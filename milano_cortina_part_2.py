# -*- coding: utf-8 -*-
"""Milano-Cortina part 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1avDLtlvsoz3DI8DLCWDRSPKQ9aGsTTz2

Now i'm going to do the topic modelling and i start from import all the libraries
"""

#import all the libraries
from textblob import TextBlob
from wordcloud import WordCloud
import pandas as pd
import numpy as np
import re
import matplotlib.pyplot as plt
import os

os.getcwd()

infile = open("positivo.txt", "r")
positivo_text = infile.read()
infile.close()

# let's print a few characters
print(positivo_text[:1800])

"""Here i start the topic modelling starting from stanza. It can help you to process the text in your language"""

import stanza

"""An advantage of stanza is very accuracy, it is very fast and it has 66 model of language"""

stanza.download('it')

stanza.download('it', package='postwita')

nlp = stanza.Pipeline('it', processors='tokenize,mwt,pos')

nlp_ita = stanza.Pipeline('it', processors='tokenize,mwt')

"""Here i print the positive tweets and i used the lemmatization because it helps you to reduce inflected forms  """

nlp = stanza.Pipeline('it', processors='tokenize,pos,lemma')
doc = nlp(positivo_text[:704])

for word in doc.iter_words():
    print(f'word: {word.text : <20} upos: {word.upos : <10} lemma: {word.lemma}')

import stanza

stanza.download('it')

nlp = stanza.Pipeline('it', processors='tokenize,mwt,pos')

doc = nlp(positivo_text)

nlp = stanza.Pipeline('it', processors='tokenize')
doc = nlp(positivo_text)

nlp = stanza.Pipeline('it', processors='tokenize,pos,lemma')
doc = nlp(positivo_text[:800])

for word in doc.iter_words():
    print(f'word:{word.text : <20} upos:{word.upos : <20}lemma: {word.lemma}')

"""Here i import gensim, which used to process raw, unstructured digital texts using unsupervised machine learning algorithms"""

import gensim

"""Here I import the libraries with nltk, gensim and latent dirichelt model"""

#import the library
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
nltk.download('punkt')
from nltk.tokenize import word_tokenize
from gensim.models import LdaModel
from gensim.corpora import Dictionary

"""In this part i added the vocabulary with lda"""

#define the italian stopwords
stop_words = set(stopwords.words('italian'))
#tokenize the text
words = word_tokenize(positivo_text)
#remove stopwords, punctuation and end of the line
filtered_words = [word for word in words if word.lower() not in stop_words and word.isalpha()]
#create a dictionary of the filtered words
dictionary = Dictionary([filtered_words])
#create a bag of words representation of the text
bow = [dictionary.doc2bow(filtered_words)]
#define the number of topic you want to extract
num_topics = 1
#LDA model with passes and iteration parameters value
lda = LdaModel(bow, num_topics = num_topics, id2word=dictionary, passes=25, iterations=100)

#print the topic
for topic in lda.print_topics():
    print(topic)

"""Now i added the topic of a given document and from the output of the cell we can infer the first document talks about the first topic"""

lda.get_document_topics(bow[0], minimum_probability = 0)

for did, doc_topics in enumerate(
    lda.get_document_topics(bow, minimum_probability = 1e-3)):
    print(f"{did : >2}: {doc_topics}")

doc2topics = []

# by setting minimum_probability = 0 we retrieve all the topics for all the docs
for doc_topics in lda.get_document_topics(bow, minimum_probability = 0):
    weights = []
    for _, w in doc_topics:
        weights.append(w)
    doc2topics.append(weights)

# let's have a look at our list
for d2t in doc2topics[:5]:
    print(d2t)

"""Now i visualize the topic modeling about the positive tweets"""

# let's focus on the first topic
topic_id = 1

# let's set the number of words we want to retrieve per topic
num_top_words = 10

# store the output of show_topics() in a dictionary of
# the form {topic : [(word_1 , weight_1), (word_2, weight_2)...]}
topics_descriptions = dict(lda.show_topics(formatted=False, num_words = num_top_words))

# it is better to re-organize the values of this dictionary in two lists:
# {topic : [[word_1, word_2, ...], [weight_1, weight_2, ...]}
topic2top_words = dict()
for topic, words in topics_descriptions.items():
    topic2top_words[topic] = [[], []]  # una lista formata da due liste
    for stats in words:
        topic2top_words[topic][0].append(stats[0])
        topic2top_words[topic][1].append(stats[1])

# draw the barplot
plt.barh(topic2top_words[topic_id][0],
         topic2top_words[topic_id][1],
         color = "#880000", edgecolor = "black")

# by default barh() plot our values in ascending order
# let's invert the axis to fix this
plt.gca().invert_yaxis()

plt.ylabel("term")
plt.title(f'Topic #{topic_id}')
plt.show()

"""In this graph we can see the most importan words of the positive tweets about the first topic and as my point of view words like: fine,milano and vinto are most used because people want to express their joy about the bid. While the other city, which co-host the olympic games is less mentioned

Now i'm going to analyze the negative tweets
"""

os.getcwd()

infile = open("negativo.txt", "r")
negativo_text = infile.read()
infile.close()

# let's print a few characters
print(negativo_text[:1800])

import gensim

#import the library
import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
nltk.download('punkt')
from nltk.tokenize import word_tokenize
from gensim.models import LdaModel
from gensim.corpora import Dictionary

#define the italian stopwords
stop_words = set(stopwords.words('italian'))
#tokenize the text
words = word_tokenize(negativo_text)
#remove stopwords, punctuation and end of the line
filtered_words = [word for word in words if word.lower() not in stop_words and word.isalpha()]
#create a dictionary of the filtered words
dictionary = Dictionary([filtered_words])
#create a bag of words representation of the text
bow = [dictionary.doc2bow(filtered_words)]
#define the number of topic you want to extract
num_topics = 1
#LDA model with passes and iteration parameters value
lda = LdaModel(bow, num_topics = num_topics, id2word=dictionary, passes=25, iterations=100)

#print the topic
for topic in lda.print_topics():
    print(topic)

"""It is interesting how in this part we find the name of the former mayor of Rome, because she didn't allow to continue the olympic bid process in 2016"""

lda.get_document_topics(bow[0], minimum_probability = 0)

for did, doc_topics in enumerate(
    lda.get_document_topics(bow, minimum_probability = 1e-3)):
    print(f"{did : >2}: {doc_topics}")

doc2topics = []

# by setting minimum_probability = 0 we retrieve all the topics for all the docs
for doc_topics in lda.get_document_topics(bow, minimum_probability = 0):
    weights = []
    for _, w in doc_topics:
        weights.append(w)
    doc2topics.append(weights)

# let's have a look at our list
for d2t in doc2topics[:5]:
    print(d2t)

# let's focus on the first topic
topic_id = 0

# let's set the number of words we want to retrieve per topic
num_top_words = 10

# store the output of show_topics() in a dictionary of
# the form {topic : [(word_1 , weight_1), (word_2, weight_2)...]}
topics_descriptions = dict(lda.show_topics(formatted=False, num_words = num_top_words))

# it is better to re-organize the values of this dictionary in two lists:
# {topic : [[word_1, word_2, ...], [weight_1, weight_2, ...]}
topic2top_words = dict()
for topic, words in topics_descriptions.items():
    topic2top_words[topic] = [[], []]  # una lista formata da due liste
    for stats in words:
        topic2top_words[topic][0].append(stats[0])
        topic2top_words[topic][1].append(stats[1])

# draw the barplot
plt.barh(topic2top_words[topic_id][0],
         topic2top_words[topic_id][1],
         color = "#880000", edgecolor = "black")

# by default barh() plot our values in ascending order
# let's invert the axis to fix this
plt.gca().invert_yaxis()

plt.ylabel("term")
plt.title(f'Topic #{topic_id}')
plt.show()

"""In this case we can see the word olimpiadi is most used than other, it is interesting we find roma because as i said before rome in 2016 had the chance to advance in their bid but the mayor disagreed

At the end of the work, I focused on the first part on sentiment analysis, which is useful to understand the thermometer of public opinion and to understand why the Olympics in Italy created a problem. After the Athens experience, many politicians, in particular the five stars, put brakes on organising big events and this also after the economic crisis of 2011. In the second part I focused on topic modelling and the related content of tweets and also the comparison between positive and negative comments
"""